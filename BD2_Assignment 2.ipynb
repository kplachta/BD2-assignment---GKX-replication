{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a316379d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Input, Dropout\n",
    "from tensorflow.keras.regularizers import L1\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Lasso\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d608c2de",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaef3f99",
   "metadata": {},
   "source": [
    "## Step 1 - loading in the data, creating training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a31371ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\pickle.py:196\u001b[0m, in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[0;32m    195\u001b[0m         warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mWarning\u001b[39;00m)\n\u001b[1;32m--> 196\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mload(handles\u001b[38;5;241m.\u001b[39mhandle)\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m excs_to_catch:\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;66;03m# e.g.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;66;03m#  \"No module named 'pandas.core.sparse.series'\"\u001b[39;00m\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;66;03m#  \"Can't get attribute '__nat_unpickle' on <module 'pandas._libs.tslib\"\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas.core.indexes.numeric'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# loading in the data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m panel \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_pickle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreturns_chars_panel.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[0;32m      3\u001b[0m macro \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_pickle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro_timeseries.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\pickle.py:201\u001b[0m, in \u001b[0;36mread_pickle\u001b[1;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[0;32m    196\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mload(handles\u001b[38;5;241m.\u001b[39mhandle)\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m excs_to_catch:\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;66;03m# e.g.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m#  \"No module named 'pandas.core.sparse.series'\"\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         \u001b[38;5;66;03m#  \"Can't get attribute '__nat_unpickle' on <module 'pandas._libs.tslib\"\u001b[39;00m\n\u001b[1;32m--> 201\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pc\u001b[38;5;241m.\u001b[39mload(handles\u001b[38;5;241m.\u001b[39mhandle, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;66;03m# e.g. can occur for files written in py27; see GH#28645 and GH#31988\u001b[39;00m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pc\u001b[38;5;241m.\u001b[39mload(handles\u001b[38;5;241m.\u001b[39mhandle, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlatin-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\compat\\pickle_compat.py:218\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fh, encoding, is_verbose)\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# \"Unpickler\" has no attribute \"is_verbose\"  [attr-defined]\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     up\u001b[38;5;241m.\u001b[39mis_verbose \u001b[38;5;241m=\u001b[39m is_verbose  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m--> 218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m up\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\pickle.py:1213\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1211\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[0;32m   1212\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, bytes_types)\n\u001b[1;32m-> 1213\u001b[0m         dispatch[key[\u001b[38;5;241m0\u001b[39m]](\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _Stop \u001b[38;5;28;01mas\u001b[39;00m stopinst:\n\u001b[0;32m   1215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stopinst\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\pickle.py:1355\u001b[0m, in \u001b[0;36m_Unpickler.load_binbytes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1352\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m \u001b[38;5;241m>\u001b[39m maxsize:\n\u001b[0;32m   1353\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnpicklingError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBINBYTES exceeds system\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms maximum size \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1354\u001b[0m                           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m bytes\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m maxsize)\n\u001b[1;32m-> 1355\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\pickle.py:298\u001b[0m, in \u001b[0;36m_Unframer.read\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_read(n)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# loading in the data\n",
    "panel = pd.read_pickle('returns_chars_panel.pkl') \n",
    "macro = pd.read_pickle('macro_timeseries.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15c86f5",
   "metadata": {},
   "source": [
    "We are assuming that the returns are already shifted by one period, i.e. each row includes values of features for t=n and in the same row the returns data is for t=n+1. If this isn't the case then the data would need to be offset as just described."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f211c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine micro and macro data\n",
    "df = pd.merge(panel,macro,on='date',how='left',suffixes=['','_macro'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bfed256",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>permno</th>\n",
       "      <th>excess_ret</th>\n",
       "      <th>ret</th>\n",
       "      <th>rfree</th>\n",
       "      <th>mvel1</th>\n",
       "      <th>beta</th>\n",
       "      <th>betasq</th>\n",
       "      <th>chmom</th>\n",
       "      <th>dolvol</th>\n",
       "      <th>...</th>\n",
       "      <th>ep_macro</th>\n",
       "      <th>b/m</th>\n",
       "      <th>crsp_spvw</th>\n",
       "      <th>svar</th>\n",
       "      <th>tbl</th>\n",
       "      <th>tms</th>\n",
       "      <th>dfy</th>\n",
       "      <th>dfr</th>\n",
       "      <th>ntis</th>\n",
       "      <th>infl</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3739444</th>\n",
       "      <td>2016-08-01</td>\n",
       "      <td>93436</td>\n",
       "      <td>-0.097265</td>\n",
       "      <td>-0.097023</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>0.948225</td>\n",
       "      <td>0.629726</td>\n",
       "      <td>0.627131</td>\n",
       "      <td>0.490549</td>\n",
       "      <td>0.993659</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.210865</td>\n",
       "      <td>0.314661</td>\n",
       "      <td>0.036571</td>\n",
       "      <td>0.000478</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0145</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>-0.031614</td>\n",
       "      <td>-0.001618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3739445</th>\n",
       "      <td>2016-09-01</td>\n",
       "      <td>93436</td>\n",
       "      <td>-0.037915</td>\n",
       "      <td>-0.037640</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>0.940011</td>\n",
       "      <td>0.654357</td>\n",
       "      <td>0.651773</td>\n",
       "      <td>0.268347</td>\n",
       "      <td>0.993663</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.201425</td>\n",
       "      <td>0.315197</td>\n",
       "      <td>0.001247</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>-0.030723</td>\n",
       "      <td>0.000918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3739446</th>\n",
       "      <td>2016-10-01</td>\n",
       "      <td>93436</td>\n",
       "      <td>-0.031253</td>\n",
       "      <td>-0.030878</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.936380</td>\n",
       "      <td>0.650269</td>\n",
       "      <td>0.647670</td>\n",
       "      <td>-0.310701</td>\n",
       "      <td>0.993629</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.192038</td>\n",
       "      <td>0.316794</td>\n",
       "      <td>0.000446</td>\n",
       "      <td>0.001673</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>-0.032543</td>\n",
       "      <td>0.002404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3739447</th>\n",
       "      <td>2016-11-01</td>\n",
       "      <td>93436</td>\n",
       "      <td>-0.042553</td>\n",
       "      <td>-0.042128</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.936096</td>\n",
       "      <td>0.638693</td>\n",
       "      <td>0.636094</td>\n",
       "      <td>-0.806963</td>\n",
       "      <td>0.993638</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.152198</td>\n",
       "      <td>0.319688</td>\n",
       "      <td>-0.017958</td>\n",
       "      <td>0.000364</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0187</td>\n",
       "      <td>0.0087</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>-0.028976</td>\n",
       "      <td>0.001247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3739448</th>\n",
       "      <td>2016-12-01</td>\n",
       "      <td>93436</td>\n",
       "      <td>0.127822</td>\n",
       "      <td>0.128247</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.929911</td>\n",
       "      <td>0.622969</td>\n",
       "      <td>0.620384</td>\n",
       "      <td>-0.540670</td>\n",
       "      <td>0.993655</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.165980</td>\n",
       "      <td>0.303286</td>\n",
       "      <td>0.035790</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>-0.027373</td>\n",
       "      <td>-0.001555</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              date  permno  excess_ret       ret     rfree     mvel1  \\\n",
       "3739444 2016-08-01   93436   -0.097265 -0.097023  0.000242  0.948225   \n",
       "3739445 2016-09-01   93436   -0.037915 -0.037640  0.000275  0.940011   \n",
       "3739446 2016-10-01   93436   -0.031253 -0.030878  0.000375  0.936380   \n",
       "3739447 2016-11-01   93436   -0.042553 -0.042128  0.000425  0.936096   \n",
       "3739448 2016-12-01   93436    0.127822  0.128247  0.000425  0.929911   \n",
       "\n",
       "             beta    betasq     chmom    dolvol  ...  ep_macro       b/m  \\\n",
       "3739444  0.629726  0.627131  0.490549  0.993659  ... -3.210865  0.314661   \n",
       "3739445  0.654357  0.651773  0.268347  0.993663  ... -3.201425  0.315197   \n",
       "3739446  0.650269  0.647670 -0.310701  0.993629  ... -3.192038  0.316794   \n",
       "3739447  0.638693  0.636094 -0.806963  0.993638  ... -3.152198  0.319688   \n",
       "3739448  0.622969  0.620384 -0.540670  0.993655  ... -3.165980  0.303286   \n",
       "\n",
       "         crsp_spvw      svar     tbl     tms     dfy     dfr      ntis  \\\n",
       "3739444   0.036571  0.000478  0.0030  0.0145  0.0094  0.0164 -0.031614   \n",
       "3739445   0.001247  0.000279  0.0030  0.0156  0.0092  0.0156 -0.030723   \n",
       "3739446   0.000446  0.001673  0.0029  0.0167  0.0090  0.0005 -0.032543   \n",
       "3739447  -0.017958  0.000364  0.0033  0.0187  0.0087  0.0051 -0.028976   \n",
       "3739448   0.035790  0.000946  0.0045  0.0222  0.0085  0.0089 -0.027373   \n",
       "\n",
       "             infl  \n",
       "3739444 -0.001618  \n",
       "3739445  0.000918  \n",
       "3739446  0.002404  \n",
       "3739447  0.001247  \n",
       "3739448 -0.001555  \n",
       "\n",
       "[5 rows x 110 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4a98a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING DATASETS\n",
    "# features + targets \n",
    "X = df.drop(columns=['ret','excess_ret','rfree','permno','date']) # everything except return info and IDs\n",
    "y = df['excess_ret']\n",
    "\n",
    "# make 20 years of training data\n",
    "date = df['date']\n",
    "training = (date <= '1977-03') # selects \n",
    "X_train, y_train = X.loc[training].values, y.loc[training].values \n",
    "\n",
    "# make 10 years of validation data\n",
    "validation = (date > '1977-03') & (date <= '1987-03') \n",
    "X_val, y_val = X.loc[validation].values, y.loc[validation].values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9929ffbe",
   "metadata": {},
   "source": [
    "## Step 2 - selecting optimal hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0353d641",
   "metadata": {},
   "source": [
    "In neural network models, both the depth of the neural network and the number of nodes in each layer is a hyperparameter. For this assignment, the depths of the neural networks are already given and we decided to stick to the number of nodes that have been used in the paper and for more meaningful comparison of results. Therefore the network architectures are as follows:\n",
    "\n",
    "NN2 architecture: 32, 16, 1\n",
    "\n",
    "NN3 architecture: 32, 16, 8, 1\n",
    "\n",
    "NN4 architecture: 32, 16, 8, 4, 1\n",
    "\n",
    "Where the numbers represent number of nodes in a layer and each layer is separated by the comma. The input layer is not represented on the above and in each case the final layer is the output layer. We follow the paper in using RelU as the activation function in hidden layers and linear in the output layer. The number of epochs is also a hyperparameter that's generally optimized, however we decided to use a model with maximum of 25 epochs and with early stop once the model stops improving for 5 consecutive passes. This was a decision we took after analysing the graphs which showed that for most models the validation performance flatlined after around 15 epochs thus we decided that 25 epochs is more than sufficient. Finally, the optimization was to pick the model which would provide smallest MSE in the validation set.\n",
    "\n",
    "We perform the most extensive search for the NN2 network given that it's the least computationally expensive and therefore takes fastest to do. For NN3 and NN4 models, we streamline the process based on our experience from optimizing the hyperparameters of the NN2 model, as we assume that in general the results will hold. We acknowledge that this isn't a perfect assumption, and a combination which yielded very bad results for the NN2 model may work well for NN3 or NN4 models. However, this is due to computational and time constraints, and we believe that we're appropriately thorough in our selection anyways. We provide further details about the hyperparameter selection process for individual models in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d936d68",
   "metadata": {},
   "source": [
    "### Optimizing NN2 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f210b8",
   "metadata": {},
   "source": [
    "The NN2 model is the least computationally expensive and therefore we test the most combinations of hyperparameters. The hyperparameters that we're adjusting are: batch size, learning rate, batch normalization and lambda for l1 regularization.\n",
    "We opt for a simplified gridsearch process, batch normalization is a binary (i.e. we either use it or don't) whereas for lr, batch size and lambda we test three different values. If we were to use full gridsearch process this would require us to run the model 2*3^3=54 times. Therefore we first test all combinations of batch size and learning rate. We then pick the best two models and run them with batch normalization and three different values of lambda. We noticed a decrease in performance, we then ran some other models with different combinations of lambda and batch normalization (not reported) and noticed that batch normalization decreases the performance of the models. Finally, we tested the two best performing models with no batch-normalization and three different learning rates and obtained the best performing model out of all combinations that we have tested. We also noticed that in the combination with batch_size = 256 and lr=1e-5 the performance was improving as lambda was decreasing so we also tried lambda = 1e-3 but it performed worse, hence why we decided that the model with lambda=1e-4 is the best performing model.\n",
    "\n",
    "Exact results are reported in a table after the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c5e982c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m2816/2816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - loss: 0.5983 - mse: 0.1176 - val_loss: 0.4982 - val_mse: 0.0602\n",
      "Epoch 2/25\n",
      "\u001b[1m2816/2816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - loss: 0.4589 - mse: 0.0368 - val_loss: 0.4114 - val_mse: 0.0388\n",
      "Epoch 3/25\n",
      "\u001b[1m2816/2816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - loss: 0.3805 - mse: 0.0251 - val_loss: 0.3348 - val_mse: 0.0312\n",
      "Epoch 4/25\n",
      "\u001b[1m2816/2816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - loss: 0.3079 - mse: 0.0210 - val_loss: 0.2668 - val_mse: 0.0283\n",
      "Epoch 5/25\n",
      "\u001b[1m2816/2816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - loss: 0.2432 - mse: 0.0195 - val_loss: 0.2093 - val_mse: 0.0271\n",
      "Epoch 6/25\n",
      "\u001b[1m2816/2816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - loss: 0.1886 - mse: 0.0188 - val_loss: 0.1616 - val_mse: 0.0267\n",
      "Epoch 7/25\n",
      "\u001b[1m2816/2816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - loss: 0.1434 - mse: 0.0186 - val_loss: 0.1238 - val_mse: 0.0266\n",
      "Epoch 8/25\n",
      "\u001b[1m2816/2816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - loss: 0.1079 - mse: 0.0185 - val_loss: 0.0948 - val_mse: 0.0265\n",
      "Epoch 9/25\n",
      "\u001b[1m2816/2816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - loss: 0.0810 - mse: 0.0185 - val_loss: 0.0733 - val_mse: 0.0265\n",
      "Epoch 10/25\n",
      "\u001b[1m2816/2816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 7ms/step - loss: 0.0612 - mse: 0.0185 - val_loss: 0.0582 - val_mse: 0.0265\n",
      "Epoch 11/25\n",
      "\u001b[1m2816/2816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - loss: 0.0473 - mse: 0.0185 - val_loss: 0.0478 - val_mse: 0.0265\n",
      "Epoch 12/25\n",
      "\u001b[1m2816/2816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - loss: 0.0379 - mse: 0.0185 - val_loss: 0.0412 - val_mse: 0.0265\n",
      "Epoch 13/25\n",
      "\u001b[1m2816/2816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - loss: 0.0320 - mse: 0.0185 - val_loss: 0.0370 - val_mse: 0.0265\n",
      "Epoch 14/25\n",
      "\u001b[1m2816/2816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - loss: 0.0281 - mse: 0.0185 - val_loss: 0.0339 - val_mse: 0.0265\n",
      "Epoch 15/25\n",
      "\u001b[1m2816/2816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 4ms/step - loss: 0.0253 - mse: 0.0185 - val_loss: 0.0316 - val_mse: 0.0265\n",
      "Epoch 16/25\n",
      "\u001b[1m2816/2816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - loss: 0.0231 - mse: 0.0185 - val_loss: 0.0298 - val_mse: 0.0265\n",
      "Epoch 17/25\n",
      "\u001b[1m2816/2816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 4ms/step - loss: 0.0214 - mse: 0.0185 - val_loss: 0.0285 - val_mse: 0.0265\n",
      "Epoch 18/25\n",
      "\u001b[1m2816/2816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - loss: 0.0202 - mse: 0.0185 - val_loss: 0.0275 - val_mse: 0.0265\n",
      "Epoch 19/25\n",
      "\u001b[1m2816/2816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - loss: 0.0194 - mse: 0.0185 - val_loss: 0.0270 - val_mse: 0.0265\n",
      "Epoch 20/25\n",
      "\u001b[1m2816/2816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - loss: 0.0189 - mse: 0.0185 - val_loss: 0.0267 - val_mse: 0.0265\n",
      "Epoch 21/25\n",
      "\u001b[1m2816/2816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - loss: 0.0187 - mse: 0.0185 - val_loss: 0.0265 - val_mse: 0.0265\n",
      "Epoch 22/25\n",
      "\u001b[1m2816/2816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - loss: 0.0185 - mse: 0.0185 - val_loss: 0.0265 - val_mse: 0.0265\n",
      "Epoch 23/25\n",
      "\u001b[1m2816/2816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - loss: 0.0185 - mse: 0.0185 - val_loss: 0.0265 - val_mse: 0.0265\n",
      "Epoch 24/25\n",
      "\u001b[1m2816/2816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - loss: 0.0185 - mse: 0.0185 - val_loss: 0.0265 - val_mse: 0.0265\n",
      "Epoch 25/25\n",
      "\u001b[1m2816/2816\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - loss: 0.0185 - mse: 0.0185 - val_loss: 0.0265 - val_mse: 0.0265\n",
      "Training MSE: 0.01851601153612137\n",
      "Validation MSE: 0.026455456390976906\n"
     ]
    }
   ],
   "source": [
    "# FINDING RIGHT HYPERPARAMETERS AND MODEL SPECIFICATION\n",
    "tf.random.set_seed(999) # setting the seed for replicatability\n",
    "\n",
    "# Hypers\n",
    "batch_size = 256\n",
    "learning_rate = 1e-5\n",
    "lamda = 1e-4\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "\n",
    "# Model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=X_train.shape[1],  # input_dim specified for the first layer\n",
    "                activation='relu',\n",
    "                kernel_regularizer = L1(lamda),\n",
    "                kernel_initializer='he_normal'))\n",
    "#model.add(BatchNormalization()) \n",
    "\n",
    "model.add(Dense(16, activation='relu',\n",
    "                kernel_regularizer = L1(lamda),\n",
    "                kernel_initializer='he_normal'))\n",
    "#model.add(BatchNormalization())  \n",
    "\n",
    "model.add(Dense(1, activation='linear',\n",
    "                kernel_regularizer = L1(lamda),\n",
    "                kernel_initializer='he_normal'))\n",
    "\n",
    "# Optimizer with specified learning rate\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "model.compile(loss='mse', \n",
    "              optimizer=optimizer,  # Use the optimizer with specified learning rate\n",
    "              metrics=['mse'])\n",
    "\n",
    "# Training the model with early stopping\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=25, \n",
    "                    batch_size=batch_size, \n",
    "                    verbose=True,\n",
    "                    callbacks=[early_stopping],\n",
    "                    validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate the model\n",
    "mse_train = model.evaluate(X_train, y_train, verbose=0)\n",
    "mse_val = model.evaluate(X_val, y_val, verbose=0)\n",
    "\n",
    "print(f\"Training MSE: {mse_train[1]}\")\n",
    "print(f\"Validation MSE: {mse_val[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c33ae2ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x28c612b5a50>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "pd.Series(history.history['mse']).plot(ax=ax, label='training mse')\n",
    "pd.Series(history.history['val_mse']).plot(ax=ax, label='validation mse')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a08397",
   "metadata": {},
   "outputs": [],
   "source": [
    "Results (no lambda and batch normalization initially):\n",
    "batch = 256, lr = 1e-3, epochs = 6/25; validation MSE = 0.02907363325357437\n",
    "batch = 512, lr = 1e-3, epochs = 9/25; validation MSE = 0.02852504514157772\n",
    "batch = 1024, lr = 1e-3, epochs = 8/25; validation MSE = 0.027601296082139015\n",
    "\n",
    "batch = 256, lr = 1e-4, epochs = 1O/25; validation MSE = 0.02680022269487381\n",
    "batch = 512, lr = 1e-4, epochs = 12/25; validation MSE = 0.026700353249907494 2nd best - round 1\n",
    "batch = 1024, lr = 1e-4, epochs = 25/25; validation MSE = 0.02671683393418789\n",
    "\n",
    "batch = 256, lr = 1e-5, epochs = 25/25; validation MSE = 0.026673775166273117 WINNER - round 1\n",
    "batch = 512, lr = 1e-5, epochs = 25/25; validation MSE = 0.026739517226815224\n",
    "batch = 1024, lr = 1e-5, epochs = 25/25; validation MSE = 0.027967121452093124\n",
    "\n",
    "Adding lambda and batch normalization to the 2 models that worked best initially lambda:\n",
    "batch = 256, lr = 1e-5, epochs = 25/25, lambda = 1e-4; validation MSE = 0.026738665997982025\n",
    "batch = 256, lr = 1e-5, epochs = 25/25, lambda = 1e-5; validation MSE = 0.027124900370836258\n",
    "batch = 256, lr = 1e-5, epochs = 25/25, lambda = 1e-6; validation MSE = 0.02796582691371441\n",
    "\n",
    "batch = 512, lr = 1e-4, epochs = 21/25, lambda = 1e-4; validation MSE = 0.03449607640504837\n",
    "batch = 512, lr = 1e-4, epochs = 25/25, lambda = 1e-5; validation MSE = 0.026746831834316254\n",
    "batch = 512, lr = 1e-4, epochs = 25/25, lambda = 1e-6; validation MSE = 0.026684550568461418 \n",
    "\n",
    "\n",
    "Lambda but no batch normalization for the 2 best models:\n",
    "batch = 256, lr = 1e-5, epochs = 25/25, lambda = 1e-3; validation MSE = 0.026455456390976906\n",
    "batch = 256, lr = 1e-5, epochs = 25/25, lambda = 1e-4; validation MSE = 0.026391010731458664 OVERALL WINNER\n",
    "batch = 256, lr = 1e-5, epochs = 25/25, lambda = 1e-5; validation MSE = 0.02645077370107174\n",
    "batch = 256, lr = 1e-5, epochs = 25/25, lambda = 1e-6; validation MSE = 0.026525743305683136\n",
    "\n",
    "batch = 512, lr = 1e-4, epochs = 25/25, lambda = 1e-4; validation MSE = 0.026469042524695396\n",
    "batch = 512, lr = 1e-4, epochs = 25/25, lambda = 1e-5; validation MSE = 0.026829596608877182\n",
    "batch = 512, lr = 1e-4, epochs = 25/25, lambda = 1e-6; validation MSE = 0.02674420364201069"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e5e49f",
   "metadata": {},
   "source": [
    "### Optimizing NN3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4f494b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\krspl\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m5840/5840\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.1173 - mse: 0.0670 - val_loss: 0.0790 - val_mse: 0.0319\n",
      "Epoch 2/15\n",
      "\u001b[1m5840/5840\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - loss: 0.0731 - mse: 0.0275 - val_loss: 0.0678 - val_mse: 0.0275\n",
      "Epoch 3/15\n",
      "\u001b[1m5840/5840\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - loss: 0.0630 - mse: 0.0247 - val_loss: 0.0582 - val_mse: 0.0266\n",
      "Epoch 4/15\n",
      "\u001b[1m5840/5840\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 8ms/step - loss: 0.0535 - mse: 0.0242 - val_loss: 0.0490 - val_mse: 0.0265\n",
      "Epoch 5/15\n",
      "\u001b[1m5840/5840\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 3ms/step - loss: 0.0446 - mse: 0.0241 - val_loss: 0.0412 - val_mse: 0.0264\n",
      "Epoch 6/15\n",
      "\u001b[1m5840/5840\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - loss: 0.0373 - mse: 0.0241 - val_loss: 0.0355 - val_mse: 0.0264\n",
      "Epoch 7/15\n",
      "\u001b[1m5840/5840\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - loss: 0.0321 - mse: 0.0241 - val_loss: 0.0318 - val_mse: 0.0264\n",
      "Epoch 8/15\n",
      "\u001b[1m5840/5840\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - loss: 0.0288 - mse: 0.0241 - val_loss: 0.0297 - val_mse: 0.0264\n",
      "Epoch 9/15\n",
      "\u001b[1m5840/5840\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - loss: 0.0271 - mse: 0.0241 - val_loss: 0.0286 - val_mse: 0.0264\n",
      "Epoch 10/15\n",
      "\u001b[1m5840/5840\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - loss: 0.0261 - mse: 0.0241 - val_loss: 0.0280 - val_mse: 0.0264\n",
      "Epoch 11/15\n",
      "\u001b[1m5840/5840\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3ms/step - loss: 0.0256 - mse: 0.0241 - val_loss: 0.0276 - val_mse: 0.0264\n",
      "Epoch 12/15\n",
      "\u001b[1m5840/5840\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3ms/step - loss: 0.0252 - mse: 0.0241 - val_loss: 0.0274 - val_mse: 0.0264\n",
      "Epoch 13/15\n",
      "\u001b[1m5840/5840\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - loss: 0.0250 - mse: 0.0241 - val_loss: 0.0272 - val_mse: 0.0264\n",
      "Epoch 14/15\n",
      "\u001b[1m5840/5840\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.0249 - mse: 0.0241 - val_loss: 0.0271 - val_mse: 0.0264\n",
      "Epoch 15/15\n",
      "\u001b[1m5840/5840\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 0.0248 - mse: 0.0241 - val_loss: 0.0271 - val_mse: 0.0264\n",
      "Training MSE: 0.02348976768553257\n",
      "Validation MSE: 0.026396850124001503\n"
     ]
    }
   ],
   "source": [
    "# FINDING RIGHT HYPERPARAMETERS AND MODEL SPECIFICATION\n",
    "tf.random.set_seed(999) # setting the seed for replicatability\n",
    "\n",
    "# Hypers\n",
    "batch_size = 256\n",
    "learning_rate = 1e-5\n",
    "lamda = 1e-4\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "\n",
    "# Model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=X_train.shape[1],  # input_dim specified for the first layer\n",
    "                activation='relu',\n",
    "                kernel_regularizer = L1(lamda),\n",
    "                kernel_initializer='he_normal'))\n",
    "\n",
    "model.add(Dense(16, activation='relu',\n",
    "                kernel_regularizer = L1(lamda),\n",
    "                kernel_initializer='he_normal'))\n",
    "\n",
    "model.add(Dense(8, activation='relu',\n",
    "                kernel_regularizer = L1(lamda),\n",
    "                kernel_initializer='he_normal'))\n",
    "\n",
    "model.add(Dense(1, activation='linear',\n",
    "                kernel_regularizer = L1(lamda),\n",
    "                kernel_initializer='he_normal'))\n",
    "\n",
    "# Optimizer with specified learning rate\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "model.compile(loss='mse', \n",
    "              optimizer=optimizer,  # Use the optimizer with specified learning rate\n",
    "              metrics=['mse'])\n",
    "\n",
    "# Training the model with early stopping\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=15, \n",
    "                    batch_size=batch_size, \n",
    "                    verbose=True,\n",
    "                    callbacks=[early_stopping],\n",
    "                    validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate the model\n",
    "mse_train = model.evaluate(X_train, y_train, verbose=0)\n",
    "mse_val = model.evaluate(X_val, y_val, verbose=0)\n",
    "\n",
    "print(f\"Training MSE: {mse_train[1]}\")\n",
    "print(f\"Validation MSE: {mse_val[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12db2e84",
   "metadata": {},
   "source": [
    "### Optimizing NN4 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f9868e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINDING RIGHT HYPERPARAMETERS AND MODEL SPECIFICATION\n",
    "tf.random.set_seed(999) # setting the seed for replicatability\n",
    "\n",
    "# Hypers\n",
    "batch_size = 256\n",
    "learning_rate = 1e-5\n",
    "lamda = 1e-6\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "\n",
    "# Model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=X_train.shape[1],  # input_dim specified for the first layer\n",
    "                activation='relu',\n",
    "                kernel_regularizer = L1(lamda),\n",
    "                kernel_initializer='he_normal'))\n",
    "\n",
    "model.add(Dense(16, activation='relu',\n",
    "                kernel_regularizer = L1(lamda),\n",
    "                kernel_initializer='he_normal'))\n",
    "\n",
    "model.add(Dense(8, activation='relu',\n",
    "                kernel_regularizer = L1(lamda),\n",
    "                kernel_initializer='he_normal'))\n",
    "\n",
    "model.add(Dense(4, activation='relu',\n",
    "                kernel_regularizer = L1(lamda),\n",
    "                kernel_initializer='he_normal'))\n",
    "\n",
    "model.add(Dense(1, activation='linear',\n",
    "                kernel_regularizer = L1(lamda),\n",
    "                kernel_initializer='he_normal'))\n",
    "\n",
    "# Optimizer with specified learning rate\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "model.compile(loss='mse', \n",
    "              optimizer=optimizer,  # Use the optimizer with specified learning rate\n",
    "              metrics=['mse'])\n",
    "\n",
    "# Training the model with early stopping\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=25, \n",
    "                    batch_size=batch_size, \n",
    "                    verbose=True,\n",
    "                    callbacks=[early_stopping],\n",
    "                    validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate the model\n",
    "mse_train = model.evaluate(X_train, y_train, verbose=0)\n",
    "mse_val = model.evaluate(X_val, y_val, verbose=0)\n",
    "\n",
    "print(f\"Training MSE: {mse_train[1]}\")\n",
    "print(f\"Validation MSE: {mse_val[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de07b5bc",
   "metadata": {},
   "source": [
    "## Step 2 - predicting returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d09c40c",
   "metadata": {},
   "source": [
    "In the previous step we have found the optimal model specification. We are now defining the optimal models inside a function for clarity of the code within the loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "059f44c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn2_model(input_dim): \n",
    "    # Hypers\n",
    "    learning_rate = 1e-5\n",
    "    lamda = 1e-4\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(input_dim,)))  # Specify input shape using Input layer\n",
    "    model.add(Dense(32, activation='relu',\n",
    "                    kernel_regularizer = L1(lamda),\n",
    "                    kernel_initializer='he_normal'))\n",
    "\n",
    "    model.add(Dense(16, activation='relu',\n",
    "                    kernel_regularizer = L1(lamda),\n",
    "                    kernel_initializer='he_normal'))\n",
    "\n",
    "    model.add(Dense(1, activation='linear',\n",
    "                    kernel_regularizer = L1(lamda),\n",
    "                    kernel_initializer='he_normal'))\n",
    "\n",
    "    # Optimizer with specified learning rate\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(loss='mse', \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=['mse'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd580692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn3_model(input_dim): \n",
    "    learning_rate = 1e-5\n",
    "    lamda = 1e-4\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(input_dim,)))  # Specify input shape using Input layer\n",
    "    model.add(Dense(32, activation='relu',\n",
    "                    kernel_regularizer = L1(lamda),\n",
    "                    kernel_initializer='he_normal'))\n",
    "\n",
    "    model.add(Dense(16, activation='relu',\n",
    "                    kernel_regularizer = L1(lamda),\n",
    "                    kernel_initializer='he_normal'))\n",
    "    \n",
    "    model.add(Dense(8, activation='relu',\n",
    "                    kernel_regularizer = L1(lamda),\n",
    "                    kernel_initializer='he_normal'))\n",
    "    \n",
    "    model.add(Dense(1, activation='linear',\n",
    "                    kernel_regularizer = L1(lamda),\n",
    "                    kernel_initializer='he_normal'))\n",
    "\n",
    "    # Optimizer with specified learning rate\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(loss='mse', \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=['mse'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4f5cb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nn4_model(input_dim): \n",
    "    learning_rate = 1e-5\n",
    "    lamda = 1e-6\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(input_dim,)))  # Specify input shape using Input layer\n",
    "    model.add(Dense(32, activation='relu',\n",
    "                    kernel_regularizer = L1(lamda),\n",
    "                    kernel_initializer='he_normal'))\n",
    "\n",
    "    model.add(Dense(16, activation='relu',\n",
    "                    kernel_regularizer = L1(lamda),\n",
    "                    kernel_initializer='he_normal'))\n",
    "    \n",
    "    model.add(Dense(8, activation='relu',\n",
    "                    kernel_regularizer = L1(lamda),\n",
    "                    kernel_initializer='he_normal'))\n",
    "\n",
    "    model.add(Dense(4, activation='relu',\n",
    "                    kernel_regularizer = L1(lamda),\n",
    "                    kernel_initializer='he_normal'))\n",
    "    \n",
    "    model.add(Dense(1, activation='linear',\n",
    "                    kernel_regularizer = L1(lamda),\n",
    "                    kernel_initializer='he_normal'))\n",
    "\n",
    "    # Optimizer with specified learning rate\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(loss='mse', \n",
    "                  optimizer=optimizer, \n",
    "                  metrics=['mse'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb8c8e0",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168369ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2631/2631 [==============================] - 1s 423us/step\n",
      "1987\n",
      "2558/2558 [==============================] - 1s 403us/step\n",
      "1988\n",
      "2525/2525 [==============================] - 1s 535us/step\n",
      "1989\n",
      "2494/2494 [==============================] - 1s 419us/step\n",
      "1990\n",
      "2551/2551 [==============================] - 1s 431us/step\n",
      "1991\n",
      "2705/2705 [==============================] - 1s 427us/step\n",
      "1992\n",
      "2989/2989 [==============================] - 1s 412us/step\n",
      "1993\n",
      "3075/3075 [==============================] - 1s 415us/step\n",
      "1994\n",
      "3248/3248 [==============================] - 1s 440us/step\n",
      "1995\n",
      "3376/3376 [==============================] - 2s 454us/step\n",
      "1996\n",
      "3337/3337 [==============================] - 2s 590us/step\n",
      "1997\n"
     ]
    }
   ],
   "source": [
    "#Getting predictions for nn2\n",
    "input_dim = X_train.shape[1]  # Number of input features\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "predictions_list_nn2 = [] \n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for year in range(1987,2016): \n",
    "    nn2_model = create_nn2_model(input_dim) # Initializing the model\n",
    "    # Selecting appropriate years for training the model\n",
    "    training = (date <= str(year)+'-12')\n",
    "    X_train, y_train = X.loc[training].values, y.loc[training].values\n",
    "\n",
    "    # Selecting the subset for which we need to predict value i.e. the next 12 months\n",
    "    prediction = (date >= str(year+1)+'-01') & (date <= str(year+1)+'-12')\n",
    "    X_pred = X.loc[prediction].values \n",
    "    \n",
    "    # Fitting the model using the training data - we pass here the optimal hyperparameters\n",
    "    nn2_model.fit(X_train, y_train, epochs=25, batch_size=256, validation_split=0.2, callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "    # Using the model for predictions\n",
    "    predictions_temp = nn2_model.predict(X_pred)\n",
    "    predictions_list_nn2.append(predictions_temp)\n",
    "    print(year)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "print(elapsed_time)\n",
    "\n",
    "predictions_array_nn2 = np.concatenate(predictions_list_nn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf3eda2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2631/2631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "1987\n",
      "\u001b[1m2558/2558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "1988\n",
      "\u001b[1m2525/2525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "1989\n",
      "\u001b[1m2494/2494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "1990\n",
      "\u001b[1m2551/2551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "1991\n",
      "\u001b[1m2705/2705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
      "1992\n",
      "\u001b[1m2989/2989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
      "1993\n",
      "\u001b[1m3075/3075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
      "1994\n",
      "\u001b[1m3248/3248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
      "1995\n",
      "\u001b[1m3376/3376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
      "1996\n",
      "\u001b[1m3337/3337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step\n",
      "1997\n",
      "\u001b[1m3155/3155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
      "1998\n",
      "\u001b[1m3088/3088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step\n",
      "1999\n",
      "\u001b[1m2884/2884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
      "2000\n",
      "\u001b[1m2688/2688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
      "2001\n",
      "\u001b[1m2535/2535\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "2002\n",
      "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "2003\n",
      "\u001b[1m2510/2510\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
      "2004\n",
      "\u001b[1m2520/2520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step\n",
      "2005\n",
      "\u001b[1m2554/2554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
      "2006\n",
      "\u001b[1m2451/2451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
      "2007\n",
      "\u001b[1m2195/2195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "2008\n",
      "\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "2009\n",
      "\u001b[1m2105/2105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "2010\n",
      "\u001b[1m2073/2073\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "2011\n",
      "\u001b[1m2056/2056\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "2012\n",
      "\u001b[1m2115/2115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "2013\n",
      "\u001b[1m2160/2160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "2014\n",
      "\u001b[1m2137/2137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "2015\n",
      "12402.323963880539\n"
     ]
    }
   ],
   "source": [
    "#Getting predictions for nn3\n",
    "input_dim = X_train.shape[1]  # Number of input features\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "predictions_list_nn3 = [] \n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for year in range(1987,2016): \n",
    "    nn3_model = create_nn3_model(input_dim) # Initializing the model\n",
    "    # Selecting appropriate years for training the model\n",
    "    training = (date <= str(year)+'-12')\n",
    "    X_train, y_train = X.loc[training].values, y.loc[training].values\n",
    "\n",
    "    # Selecting the subset for which we need to predict value i.e. the next 12 months\n",
    "    prediction = (date >= str(year+1)+'-01') & (date <= str(year+1)+'-12')\n",
    "    X_pred = X.loc[prediction].values \n",
    "    \n",
    "    # Fitting the model using the training data\n",
    "    nn3_model.fit(X_train, y_train, epochs=15, batch_size=256, validation_split=0.2, verbose=0)\n",
    "\n",
    "    # Using the model for predictions\n",
    "    predictions_temp = nn3_model.predict(X_pred)\n",
    "    predictions_list_nn3.append(predictions_temp)\n",
    "    print(year)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "print(elapsed_time)\n",
    "\n",
    "predictions_array_nn3 = np.concatenate(predictions_list_nn3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99df9f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2631/2631\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "1987\n",
      "\u001b[1m2558/2558\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
      "1988\n",
      "\u001b[1m2525/2525\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
      "1989\n",
      "\u001b[1m2494/2494\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "1990\n",
      "\u001b[1m2551/2551\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "1991\n",
      "\u001b[1m2705/2705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
      "1992\n",
      "\u001b[1m2989/2989\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
      "1993\n",
      "\u001b[1m3075/3075\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
      "1994\n",
      "\u001b[1m3248/3248\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
      "1995\n",
      "\u001b[1m3376/3376\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step\n",
      "1996\n",
      "\u001b[1m3337/3337\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step\n",
      "1997\n",
      "\u001b[1m3155/3155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step\n",
      "1998\n",
      "\u001b[1m3088/3088\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
      "1999\n",
      "\u001b[1m2884/2884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step\n",
      "2000\n",
      "\u001b[1m2688/2688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "2001\n",
      "\u001b[1m2535/2535\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "2002\n",
      "\u001b[1m2497/2497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "2003\n",
      "\u001b[1m2510/2510\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "2004\n",
      "\u001b[1m2520/2520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
      "2005\n",
      "\u001b[1m2554/2554\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "2006\n",
      "\u001b[1m2451/2451\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "2007\n",
      "\u001b[1m2195/2195\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "2008\n",
      "\u001b[1m2134/2134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "2009\n",
      "\u001b[1m2105/2105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "2010\n",
      "\u001b[1m2073/2073\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "2011\n",
      "\u001b[1m2056/2056\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
      "2012\n",
      "\u001b[1m2115/2115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "2013\n",
      "\u001b[1m2160/2160\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "2014\n",
      "\u001b[1m2137/2137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "2015\n",
      "12655.355174779892\n"
     ]
    }
   ],
   "source": [
    "#Getting predictions for nn4\n",
    "input_dim = X_train.shape[1]  # Number of input features\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "predictions_list_nn4 = [] \n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for year in range(1987,2016): \n",
    "    nn4_model = create_nn4_model(input_dim) # Initializing the model\n",
    "    # Selecting appropriate years for training the model\n",
    "    training = (date <= str(year)+'-12')\n",
    "    X_train, y_train = X.loc[training].values, y.loc[training].values\n",
    "\n",
    "    # Selecting the subset for which we need to predict value i.e. the next 12 months\n",
    "    prediction = (date >= str(year+1)+'-01') & (date <= str(year+1)+'-12')\n",
    "    X_pred = X.loc[prediction].values \n",
    "    \n",
    "    # Fitting the model using the training data\n",
    "    nn4_model.fit(X_train, y_train, epochs=15, batch_size=256, validation_split=0.2, verbose=0)\n",
    "\n",
    "    # Using the model for predictions\n",
    "    predictions_temp = nn4_model.predict(X_pred)\n",
    "    predictions_list_nn4.append(predictions_temp)\n",
    "    print(year)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "print(elapsed_time)\n",
    "\n",
    "predictions_array_nn4 = np.concatenate(predictions_list_nn4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3a3637",
   "metadata": {},
   "source": [
    "### NN2 R-squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "70f4cfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with actual returns and predictions only for the testing period timeframe\n",
    "evaluation_range = (date >= '1988-01') & (date <= '2016-12')\n",
    "evaluation_df = df[['date','permno', 'excess_ret']].copy()\n",
    "evaluation_df = evaluation_df.loc[evaluation_range]\n",
    "evaluation_df['pred_y'] = predictions_array_nn2\n",
    "evaluation_df.to_csv('Q2_nn2_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ce23b220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0048500523400141216\n"
     ]
    }
   ],
   "source": [
    "# Creating columns which contain the elements to be summed for the OOS R-squared formula\n",
    "evaluation_df['MSE'] = (evaluation_df['excess_ret'] - evaluation_df['pred_y'])** 2\n",
    "evaluation_df['excess_ret_sq'] = evaluation_df['excess_ret'] ** 2\n",
    "\n",
    "#Calculating the R-squared for all companies combined\n",
    "R_sq = 1 - sum(evaluation_df['MSE']) / sum(evaluation_df['excess_ret_sq'])\n",
    "print(R_sq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dba34eb",
   "metadata": {},
   "source": [
    "### NN3 R-squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03a9f3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with actual returns and predictions only for the testing period timeframe\n",
    "evaluation_range = (date >= '1988-01') & (date <= '2016-12')\n",
    "evaluation_df_nn3 = df[['date','permno', 'excess_ret']].copy()\n",
    "evaluation_df_nn3 = evaluation_df_nn3.loc[evaluation_range]\n",
    "evaluation_df_nn3['pred_y'] = predictions_array_nn3\n",
    "evaluation_df_nn3.to_csv('Q2_nn3_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdbec06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0004915190023231375\n"
     ]
    }
   ],
   "source": [
    "# Creating columns which contain the elements to be summed for the OOS R-squared formula\n",
    "evaluation_df_nn3['MSE'] = (evaluation_df_nn3['excess_ret'] - evaluation_df_nn3['pred_y'])** 2\n",
    "evaluation_df_nn3['excess_ret_sq'] = evaluation_df_nn3['excess_ret'] ** 2\n",
    "\n",
    "#Calculating the R-squared for all companies combined\n",
    "R_sq_nn3 = 1 - sum(evaluation_df_nn3['MSE']) / sum(evaluation_df_nn3['excess_ret_sq'])\n",
    "print(R_sq_nn3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67e003b",
   "metadata": {},
   "source": [
    "### NN4 R-Squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b112bde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0008382474246113603\n"
     ]
    }
   ],
   "source": [
    "# Creating a dataframe with actual returns and predictions only for the testing period timeframe\n",
    "evaluation_range = (date >= '1988-01') & (date <= '2016-12')\n",
    "evaluation_df_nn4 = df[['date','permno', 'excess_ret']].copy()\n",
    "evaluation_df_nn4 = evaluation_df_nn4.loc[evaluation_range]\n",
    "evaluation_df_nn4['pred_y'] = predictions_array_nn4\n",
    "evaluation_df_nn4.to_csv('Q2_nn4_results.csv', index=False)\n",
    "\n",
    "# Creating columns which contain the elements to be summed for the OOS R-squared formula\n",
    "evaluation_df_nn4['MSE'] = (evaluation_df_nn4['excess_ret'] - evaluation_df_nn4['pred_y'])** 2\n",
    "evaluation_df_nn4['excess_ret_sq'] = evaluation_df_nn4['excess_ret'] ** 2\n",
    "\n",
    "#Calculating the R-squared for all companies combined\n",
    "R_sq_nn4 = 1 - sum(evaluation_df_nn4['MSE']) / sum(evaluation_df_nn4['excess_ret_sq'])\n",
    "print(R_sq_nn4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f913741",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b215f755",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_nn3(X, y, start, end):\n",
    "    input_dim = X.shape[1]  # Number of input features\n",
    "\n",
    "    predictions_list = []\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "    for year in range(start, end): \n",
    "        nn3_model = create_nn3_model(input_dim) # Initializing the model\n",
    "        # Selecting appropriate years for training the model\n",
    "        training = (date <= str(year)+'-12')\n",
    "        X_train, y_train = X.loc[training].values, y.loc[training].values\n",
    "\n",
    "        # Selecting the subset for which we need to predict value i.e. the next 12 months\n",
    "        prediction = (date >= str(year+1)+'-01') & (date <= str(year+1)+'-12')\n",
    "        X_pred = X.loc[prediction].values \n",
    "\n",
    "        # Fitting the model using the training data\n",
    "        nn3_model.fit(X_train, y_train, epochs=15, batch_size=256, validation_split=0.2, callbacks=[early_stopping], verbose=0)\n",
    "\n",
    "        # Using the model for predictions\n",
    "        predictions_temp = nn3_model.predict(X_pred)\n",
    "        predictions_list.append(predictions_temp)\n",
    "        \n",
    "        del nn3_model, X_train, y_train, X_pred, predictions_temp\n",
    "        gc.collect()\n",
    "\n",
    "        \n",
    "    predictions_array = np.concatenate(predictions_list)\n",
    "    return predictions_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0f41ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oos_rsq(predictions, start, end):\n",
    "    # Creating a dataframe with actual returns and predictions only for the testing period timeframe\n",
    "    date_range = (date >= str(start+1)+'-01') & (date <= str(end+1)+'-12')\n",
    "    evaluation_df = df_q3[['excess_ret']].loc[date_range].copy()\n",
    "    evaluation_df['pred_y'] = predictions\n",
    "      \n",
    "    # Creating columns which contain the elements to be summed for the OOS R-squared formula\n",
    "    evaluation_df['MSE'] = (evaluation_df['excess_ret'] - evaluation_df['pred_y'])** 2\n",
    "    evaluation_df['excess_ret_sq'] = evaluation_df['excess_ret'] ** 2\n",
    "    \n",
    "    #Calculating the R-squared for all companies combined\n",
    "    R_sq = 1 - sum(evaluation_df['MSE']) / sum(evaluation_df['excess_ret_sq'])\n",
    "    return R_sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c53c9baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_filter_q3 = (date >= '1990-01') & (date <= '2003-12')\n",
    "df_q3 = df.loc[date_filter_q3]\n",
    "\n",
    "# Apply the filter to X and y\n",
    "X_q3 = df_q3.drop(columns=['ret', 'excess_ret', 'rfree', 'permno', 'date']) # everything except return info and IDs\n",
    "y_q3 = df_q3['excess_ret']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ef5ae18",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2884/2884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
      "\u001b[1m2688/2688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
      "\u001b[1m2535/2535\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step\n",
      "Finished column: 103 Secodns elapsed on col:288.5442657470703\n",
      "\u001b[1m2884/2884\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1ms/step\n",
      "\u001b[1m2688/2688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "\u001b[1m2535/2535\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "Finished column: 104 Secodns elapsed on col:275.1575810909271\n"
     ]
    }
   ],
   "source": [
    "R_squared_list = []\n",
    "\n",
    "for column in range(0, 105):\n",
    "    time_s = time.time()\n",
    "    \n",
    "    column_label = X_q3.columns[column]\n",
    "    temp_X = X_q3.drop(column_label, axis=1)\n",
    "\n",
    "    temp_predictions = get_predictions_nn3(temp_X, y_q3, 2000, 2003)\n",
    "    temp_r_sq = get_oos_rsq(temp_predictions, 2000, 2003)\n",
    "    \n",
    "    R_squared_list.append(temp_r_sq)\n",
    "    \n",
    "    del temp_X, temp_predictions, temp_r_sq\n",
    "    gc.collect()\n",
    "    \n",
    "    time_e = time.time()\n",
    "    print('Finished column: '+str(column), '  |Seconds elapsed on col:' + str(time_e-time_s))\n",
    "\n",
    "\n",
    "# Convert the list of arrays (predictions_list) to a DataFrame\n",
    "R_squared_df = pd.DataFrame(R_squared_list, columns=['R_squared'])\n",
    "\n",
    "R_squared_df.to_csv('Q3_R_sq.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c16166",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bf4508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise\n",
    "lasso = Lasso()\n",
    "\n",
    "# hyperparameter panel to tune\n",
    "param_grid_lasso = {'alpha': [0.1, 0.5, 1, 5, 10]}\n",
    "# gridsearch for hyperparameter tuning\n",
    "grid_lasso = GridSearchCV(estimator=lasso, param_grid=param_grid_lasso, scoring='neg_mean_squared_error', cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc811ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lasso results\n",
    "grid_result_lasso = grid_lasso.fit(X_train, y_train)\n",
    "\n",
    "# optimal hyperparameters\n",
    "best_alpha = grid_result_lasso.best_params_['alpha']\n",
    "lasso_best = Lasso(alpha=best_alpha)\n",
    "lasso_best.fit(X_train, y_train)\n",
    "y_pred_lasso = lasso_best.predict(X_test)\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "r2_lasso = 1 - (mse_lasso / np.var(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e8dd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare Lasso with NN results\n",
    "\n",
    "print(f\"LASSO OOS MSE: {mse_lasso}\")\n",
    "print(f\"LASSO OOS R²: {r2_lasso}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d34ddb",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c2bbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting OOS R-squared for each company\n",
    "unique_company = evaluation_df['permno'].unique()\n",
    "R_sq_list = []\n",
    "\n",
    "for company in unique_company:\n",
    "    company_df = evaluation_df[evaluation_df['permno'] == company]\n",
    "    mse_sum = company_df['MSE'].sum()\n",
    "    ret_sq_sum = company_df['excess_ret_sq'].sum()\n",
    "    \n",
    "    # Ensure no division by zero\n",
    "    if ret_sq_sum == 0:\n",
    "        R_sq_temp = float('nan')  # or handle as appropriate\n",
    "    else:\n",
    "        R_sq_temp = 1 - mse_sum / ret_sq_sum\n",
    "    \n",
    "    R_sq_list.append((company, R_sq_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70493ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Big Data 2 assignment code records\n",
    "# created by Adelina C.ZF. at 7/6/2024\n",
    "# A second approach to solve the assignment\n",
    "# using grid search for hyperparameter tuning instead of manual comparison. did not use due to hardware constraints\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Lasso\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#%% Load data\n",
    "# import datasets from provided files\n",
    "panel = pd.read_pickle('returns_chars_panel.pkl') \n",
    "macro = pd.read_pickle('macro_timeseries.pkl')\n",
    "# merge micro and macro dataset\n",
    "df = pd.merge(panel,macro,on='date',how='left',suffixes=['','_macro'])\n",
    "\n",
    "#%% train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=1)  # set seed for reproducability\n",
    "\n",
    "#%% Hyperparameter tuning\n",
    "# NN model with various hidden layers - self-defined function\n",
    "def create_model(neurons=64, dropout_rate=0.2, layers=2):\n",
    "    model = Sequential()  # note randomness here at initialisation - double check later for optimal results\n",
    "    model.add(Dense(neurons, input_dim=X_train.shape[1], activation='relu'))  #take inputs from parameter panel for tuning later\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    for _ in range(layers-1):\n",
    "        model.add(Dense(neurons, activation='relu'))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])  # adam optimiser, loss function take MSE\n",
    "    return model\n",
    "\n",
    "# import keras regressor to implement self-defined function above\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "# fit model\n",
    "model_nn = KerasRegressor(build_fn=create_model, verbose=0)\n",
    "param_grid = {\n",
    "    'batch_size': [216, 512, 1024],   ####note might need update after running to search for optimal results\n",
    "    'epochs': [25, 50],\n",
    "    'neurons': [64, 128],\n",
    "    'dropout_rate': [0.0, 0.2],\n",
    "    'layers': [2, 3, 4]\n",
    "}\n",
    "\n",
    "#%% hyperparameter tuning using gridsearch powered by keras regressor\n",
    "grid_nn = GridSearchCV(estimator=model_nn, param_grid=param_grid, scoring='neg_mean_squared_error', cv=3)\n",
    "grid_result_nn = grid_nn.fit(X_train, y_train)\n",
    "\n",
    "# tuning results\n",
    "best_params = grid_result_nn.best_params_\n",
    "best_model = create_model(neurons=best_params['neurons'], dropout_rate=best_params['dropout_rate'], layers=best_params['layers'])\n",
    "best_model.fit(X_train, y_train, epochs=best_params['epochs'], batch_size=best_params['batch_size'], verbose=0)\n",
    "\n",
    "# OOS results\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse_oos = mean_squared_error(y_test, y_pred)\n",
    "r2_oos = 1 - (mse_oos / np.var(y_test))\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"OOS MSE: {mse_oos}\")\n",
    "print(f\"OOS R²: {r2_oos}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
